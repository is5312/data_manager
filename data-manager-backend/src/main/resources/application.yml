spring:
  application:
    name: data-manager-backend
  
  servlet:
    multipart:
      # Allow large CSV/GZIP uploads (batch job will process asynchronously)
      max-file-size: 2GB
      max-request-size: 2GB

  datasource:
    url: jdbc:postgresql://localhost:5432/datamanager?reWriteBatchedInserts=true
    username: postgres
    password: changeme
    driver-class-name: org.postgresql.Driver
    hikari:
      maximum-pool-size: 20
      connection-timeout: 30000
      idle-timeout: 600000
      max-lifetime: 1800000
  
  # JPA/Hibernate Configuration
  jpa:
    database-platform: org.hibernate.dialect.PostgreSQLDialect
    hibernate:
      ddl-auto: validate  # Use Flyway for schema management
    properties:
      hibernate:
        format_sql: true
        show_sql: false
        use_sql_comments: true
    show-sql: false
    open-in-view: false
  
  # Flyway Configuration
  flyway:
    enabled: true
    baseline-on-migrate: true
    locations: classpath:db/migration
    schemas: public
    validate-on-migrate: true
  
  sql:
    init:
      mode: never  # Disable since we're using Flyway

  # MVC Async Configuration
  # For streaming large datasets (Arrow/CSV), we need extended timeout
  # -1 means no timeout (use with caution), or set to high value (e.g., 600000 = 10 minutes)
  mvc:
    async:
      request-timeout: 600000  # 10 minutes timeout for streaming endpoints

  # Spring Batch (store metadata tables in Postgres)
  batch:
    job:
      enabled: false
    jdbc:
      initialize-schema: always

# JOOQ Configuration
jooq:
  sql-dialect: POSTGRES

# Logging
logging:
  level:
    com.datamanager.backend: DEBUG
    org.springframework.jdbc.core: DEBUG
    org.springframework.batch: DEBUG
    org.jooq: DEBUG
    org.hibernate.SQL: DEBUG

server:
  tomcat:
    # Avoid connection resets for large uploads
    max-swallow-size: -1

